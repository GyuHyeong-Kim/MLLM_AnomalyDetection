{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfd1e7-7ae0-45b2-be6a-6a91aa47d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "overlay_paths = sorted(glob.glob(\"/home/s2behappy4/data/gyuhyeong/code/bridge_data/**/*.png\", recursive=True))\n",
    "print(f\"Found {len(overlay_paths)} overlay images\")\n",
    "\n",
    "loader = DataLoader(overlay_paths, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "all_tokens = []\n",
    "with torch.no_grad():\n",
    "    for batch_paths in loader:\n",
    "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        vision_out   = model.vision_model(pixel_values=inputs.pixel_values)\n",
    "        img_embeds   = vision_out.last_hidden_state        \n",
    "\n",
    "        batch_queries   = model.query_tokens.expand(img_embeds.size(0), -1, -1)\n",
    "        qf_out          = model.qformer(\n",
    "            query_embeds           = batch_queries,\n",
    "            encoder_hidden_states  = img_embeds,\n",
    "            encoder_attention_mask = torch.ones(img_embeds.size()[:-1], device=device, dtype=torch.long),\n",
    "            return_dict            = True,\n",
    "        )\n",
    "        mask_tokens     = qf_out.last_hidden_state.mean(dim=1)  \n",
    "\n",
    "        all_tokens.append(mask_tokens.cpu())\n",
    "\n",
    "all_tokens = torch.cat(all_tokens, dim=0)  \n",
    "torch.save(all_tokens, \"/home/s2behappy4/data/gyuhyeong/code/bridge_data/mask_token_single.pt\")\n",
    "print(\"✅ mask_token_single.pt saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f0597-ff1b-4a4f-9b6b-c8aea8b8c8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
    "    trust_remote_code  = True,\n",
    "    torch_dtype        = torch.bfloat16,\n",
    "    low_cpu_mem_usage  = True,\n",
    "    device_map         = \"auto\",\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e57e1fb-1e93-4e79-b73f-edf2ddb9b408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Avg Loss: 0.0715\n",
      "Epoch 2 — Avg Loss: 0.0131\n",
      "Epoch 3 — Avg Loss: 0.0108\n",
      "Epoch 4 — Avg Loss: 0.0098\n",
      "Epoch 5 — Avg Loss: 0.0092\n",
      "✅ bridge weights saved\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, SiglipImageProcessor\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "sig_processor = SiglipImageProcessor.from_pretrained(\n",
    "    \"google/siglip-base-patch16-224\"\n",
    ")\n",
    "\n",
    "mask_tokens = torch.load(\n",
    "    \"/home/s2behappy4/data/gyuhyeong/code/bridge_data/mask_token_single.pt\"\n",
    ").to(device).float()  \n",
    "\n",
    "orig_paths = sorted(glob.glob(\n",
    "    \"/home/s2behappy4/data/gyuhyeong/code/bridge_data/**/*.png\",\n",
    "    recursive=True\n",
    "))\n",
    "\n",
    "feat_list = []\n",
    "batch_size = 16\n",
    "\n",
    "for i in range(0, len(orig_paths), batch_size):\n",
    "    batch_paths = orig_paths[i : i + batch_size]\n",
    "    imgs = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "\n",
    "    inputs = sig_processor(\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        do_resize=True,\n",
    "        size={\"height\": 224, \"width\": 224},\n",
    "    ).pixel_values  \n",
    "\n",
    "    if inputs.dim() == 5:\n",
    "        inputs = inputs.squeeze(1)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vis_out = model.vision_tower(\n",
    "            pixel_values=inputs,\n",
    "            interpolate_pos_encoding=True\n",
    "        )\n",
    "    img_feats = vis_out.last_hidden_state.mean(dim=1)  \n",
    "    feat_list.append(img_feats.cpu())\n",
    "\n",
    "image_feats = torch.cat(feat_list, dim=0).float().to(device)  \n",
    "assert mask_tokens.size(0) == image_feats.size(0), \"Error\"\n",
    "\n",
    "bridge = nn.Linear(768, model.vision_tower.config.hidden_size).to(device)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "bridge.train()\n",
    "\n",
    "ds        = TensorDataset(mask_tokens, image_feats)\n",
    "loader    = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(bridge.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    for m_tok, img_f in loader:\n",
    "        pred = bridge(m_tok)\n",
    "        loss = criterion(pred, img_f)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} — Avg Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "torch.save(\n",
    "    bridge.state_dict(),\n",
    "    \"/home/s2behappy4/data/gyuhyeong/code/bridge_data/bridge_weights_train.pt\"\n",
    ")\n",
    "print(\"✅ bridge weights saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
