{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3912571-2389-4a71-bae0-4ce4aacb38c5",
   "metadata": {},
   "source": [
    "# # Fourier Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f12de7-035b-4af9-a5c0-7cc5b482e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FourierPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, height: int, width: int):\n",
    "        super().__init__()\n",
    "        y_coords = torch.arange(height, dtype=torch.float32).unsqueeze(1)\n",
    "        x_coords = torch.arange(width, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        y_coords = y_coords / (height - 1) * 2 * math.pi\n",
    "        x_coords = x_coords / (width - 1) * 2 * math.pi\n",
    "\n",
    "        num_bands = hidden_dim // 4\n",
    "        bands = torch.pow(2.0, torch.arange(num_bands, dtype=torch.float32))\n",
    "\n",
    "        y_sin = torch.sin(y_coords * bands).unsqueeze(2)\n",
    "        y_cos = torch.cos(y_coords * bands).unsqueeze(2)\n",
    "        x_sin = torch.sin(x_coords * bands).unsqueeze(1)\n",
    "        x_cos = torch.cos(x_coords * bands).unsqueeze(1)\n",
    "\n",
    "        y_encoding = torch.cat([y_sin, y_cos], dim=2).flatten(2) \n",
    "        x_encoding = torch.cat([x_sin, x_cos], dim=2).flatten(2) \n",
    "\n",
    "        self.register_buffer('pos_encoding', y_encoding + x_encoding)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f04481-1b14-404d-9774-030b2b47bb24",
   "metadata": {},
   "source": [
    "# # Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52b8747-5a32-4148-aaeb-9ce46cf05877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, query, mask_pixels):\n",
    "        attn_output, _ = self.cross_attn(query, mask_pixels, mask_pixels)\n",
    "        query = self.norm1(query + attn_output) \n",
    "\n",
    "        attn_output, _ = self.self_attn(query, query, query)\n",
    "        query = self.norm2(query + attn_output) \n",
    "\n",
    "        ffn_output = self.ffn(query)\n",
    "        query = self.norm3(query + ffn_output) \n",
    "        \n",
    "        return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5d1f7-ac42-4f3a-93fb-72281dfff313",
   "metadata": {},
   "source": [
    "# # Mask Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d868141-3cbb-4aa1-9ae4-d5e988d0b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=256, n_heads=8, n_queries=4, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.learnable_queries = nn.Parameter(torch.randn(1, n_queries, embed_dim))\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [TransformerDecoderLayer(embed_dim, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, all_pixel_features, mask):\n",
    "        mask_pixels = all_pixel_features[mask]\n",
    "\n",
    "        if mask_pixels.shape[0] == 0:\n",
    "            return torch.zeros(1, self.embed_dim, device=all_pixel_features.device)\n",
    "\n",
    "        queries = self.learnable_queries.expand(1, -1, -1)\n",
    "        for layer in self.decoder_layers:\n",
    "            queries = layer(queries, mask_pixels.unsqueeze(0))\n",
    "\n",
    "        mask_token = queries.mean(dim=1)\n",
    "        \n",
    "        return mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3a7c-6af8-4806-9a87-9f87c5bb9607",
   "metadata": {},
   "source": [
    "# # Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d410de4-3ac8-4614-8516-c4865f869b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim // 2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(output_dim // 2, output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781cf6c-d948-49fe-a94a-dc4fb2aba4bf",
   "metadata": {},
   "source": [
    "# # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3b102-ee4d-4dc5-8b64-3f46d0ccb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class PixelProjectionLayer(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_dim, output_dim, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "class FourierPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, height: int, width: int):\n",
    "        super().__init__()\n",
    "        pos_encoding = torch.zeros(1, hidden_dim, height, width)\n",
    "        y_coords = torch.arange(height, dtype=torch.float32).unsqueeze(1).mul(math.pi / (height - 1))\n",
    "        x_coords = torch.arange(width, dtype=torch.float32).unsqueeze(0).mul(math.pi / (width - 1))\n",
    "        bands = torch.pow(2.0, torch.arange(hidden_dim // 4, dtype=torch.float32))\n",
    "        y_sin, y_cos = torch.sin(y_coords * bands), torch.cos(y_coords * bands)\n",
    "        x_sin, x_cos = torch.sin(x_coords * bands), torch.cos(x_coords * bands)\n",
    "        y_enc = torch.cat([y_sin, y_cos], dim=-1).unsqueeze(1).expand(-1, width, -1)\n",
    "        x_enc = torch.cat([x_sin, x_cos], dim=-1).unsqueeze(0).expand(height, -1, -1)\n",
    "        pos = torch.cat([y_enc, x_enc], dim=-1)\n",
    "        pos_encoding[0] = pos.permute(2, 0, 1)\n",
    "        self.register_buffer('pos_encoding', pos_encoding, persistent=False)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, embed_dim * 4), nn.GELU(), nn.Linear(embed_dim * 4, embed_dim))\n",
    "        self.norm1, self.norm2, self.norm3 = nn.LayerNorm(embed_dim), nn.LayerNorm(embed_dim), nn.LayerNorm(embed_dim)\n",
    "    def forward(self, query, mask_pixels):\n",
    "        attn_output, _ = self.cross_attn(query=query, key=mask_pixels, value=mask_pixels)\n",
    "        query = self.norm1(query + attn_output)\n",
    "        attn_output, _ = self.self_attn(query=query, key=query, value=query)\n",
    "        query = self.norm2(query + attn_output)\n",
    "        ffn_output = self.ffn(query)\n",
    "        query = self.norm3(query + ffn_output)\n",
    "        return query\n",
    "\n",
    "class MaskTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=256, n_heads=8, n_queries=4, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.learnable_queries = nn.Parameter(torch.randn(1, n_queries, embed_dim))\n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(embed_dim, n_heads) for _ in range(n_layers)])\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, all_pixel_features, mask):\n",
    "        mask_pixels = all_pixel_features[mask]\n",
    "        if mask_pixels.shape[0] == 0: return torch.zeros(1, self.embed_dim, device=all_pixel_features.device)\n",
    "        queries = self.learnable_queries.expand(1, -1, -1)\n",
    "        for layer in self.decoder_layers:\n",
    "            queries = layer(queries, mask_pixels.unsqueeze(0))\n",
    "        return queries.mean(dim=1)\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim + output_dim) // 2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear((input_dim + output_dim) // 2, output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_norm(self.fc2(self.activation(self.fc1(x))))\n",
    "\n",
    "class AnomalyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.variant_folders = sorted([os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self): return len(self.variant_folders)\n",
    "    def __getitem__(self, idx):\n",
    "        folder_path = self.variant_folders[idx]\n",
    "        image = self.transform(Image.open(os.path.join(folder_path, \"image.png\")).convert(\"RGB\"))\n",
    "        with open(os.path.join(folder_path, \"original_masks.pkl\"), \"rb\") as f: masks = pickle.load(f)\n",
    "        with open(os.path.join(folder_path, \"labels.json\"), \"r\") as f: labels = json.load(f)\n",
    "        return image, masks, labels\n",
    "\n",
    "def train():\n",
    "    TRAIN_CONFIG = {\n",
    "        \"data_root_dir\": \"/home/s2behappy4/data/gyuhyeong/MLLM_Anomaly/Demo_data/\",\n",
    "        \"target_embedding_path\": \"target_embeddings_grid.pt\",\n",
    "        \"image_size\": 512,\n",
    "        \"embed_dim\": 256,\n",
    "        \"llm_hidden_dim\": 4096,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 20, \n",
    "        \"model_save_path\": \"/home/s2behappy4/data/gyuhyeong/MLLM_Anomaly/checkpoints/anomaly_detector_v1.pth\"\n",
    "    }\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"--- Starting Training on {device} ---\")\n",
    "    \n",
    "    target_embeddings = torch.load(TRAIN_CONFIG[\"target_embedding_path\"])\n",
    "    target_normal_emb = target_embeddings[\"normal\"].to(device)\n",
    "    target_anomaly_emb = target_embeddings[\"anomaly\"].to(device)\n",
    "    \n",
    "    pixel_proj_layer = PixelProjectionLayer(output_dim=TRAIN_CONFIG[\"embed_dim\"]).to(device)\n",
    "    pos_encoding = FourierPositionalEncoding(TRAIN_CONFIG[\"embed_dim\"], TRAIN_CONFIG[\"image_size\"], TRAIN_CONFIG[\"image_size\"]).to(device)\n",
    "    mask_tokenizer = MaskTokenizer(embed_dim=TRAIN_CONFIG[\"embed_dim\"]).to(device)\n",
    "    projector = Projector(input_dim=TRAIN_CONFIG[\"embed_dim\"], output_dim=TRAIN_CONFIG[\"llm_hidden_dim\"]).to(device)\n",
    "\n",
    "    dataset = AnomalyDataset(TRAIN_CONFIG[\"data_root_dir\"])\n",
    "    dataloader = DataLoader(dataset, batch_size=TRAIN_CONFIG[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    loss_fn = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    params_to_train = list(pixel_proj_layer.parameters()) + list(mask_tokenizer.parameters()) + list(projector.parameters())\n",
    "    optimizer = torch.optim.Adam(params_to_train, lr=TRAIN_CONFIG[\"learning_rate\"])\n",
    "    \n",
    "    for epoch in range(TRAIN_CONFIG[\"epochs\"]):\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{TRAIN_CONFIG['epochs']}\")\n",
    "        for batch_images, batch_masks, batch_labels in loop:\n",
    "            batch_images = batch_images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pixel_features_base = pixel_proj_layer(batch_images)\n",
    "            pixel_features_pos = pos_encoding(pixel_features_base).permute(0, 2, 3, 1) \n",
    "            \n",
    "            total_loss = 0\n",
    "            \n",
    "            for i in range(len(batch_images)):\n",
    "                image_features, masks, labels = pixel_features_pos[i], batch_masks[i], batch_labels[i]\n",
    "\n",
    "                for mask_idx_str, label in labels.items():\n",
    "                    mask = torch.from_numpy(masks[int(mask_idx_str)]).to(device)\n",
    "                    predicted_embedding = projector(mask_tokenizer(image_features, mask))\n",
    "                    target_embedding = target_normal_emb if label == \"normal\" else target_anomaly_emb\n",
    "                    \n",
    "                    loss_target = torch.ones(predicted_embedding.shape[0]).to(device)\n",
    "                    total_loss += loss_fn(predicted_embedding, target_embedding.unsqueeze(0), loss_target)\n",
    "            \n",
    "            if isinstance(total_loss, torch.Tensor):\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                loop.set_postfix(loss=total_loss.item())\n",
    "    \n",
    "    print(\"--- Training finished. Saving model... ---\")\n",
    "    os.makedirs(os.path.dirname(TRAIN_CONFIG[\"model_save_path\"]), exist_ok=True)\n",
    "    torch.save({\n",
    "        'pixel_proj_layer_state_dict': pixel_proj_layer.state_dict(),\n",
    "        'mask_tokenizer_state_dict': mask_tokenizer.state_dict(),\n",
    "        'projector_state_dict': projector.state_dict(),\n",
    "    }, TRAIN_CONFIG[\"model_save_path\"])\n",
    "    print(f\"Model saved successfully to {TRAIN_CONFIG['model_save_path']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
