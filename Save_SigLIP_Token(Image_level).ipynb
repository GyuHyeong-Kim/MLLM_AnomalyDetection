{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec9cc42-96f4-4ec2-a4c5-fb46743027a3",
   "metadata": {},
   "source": [
    "# # SAM2 & SigLIP load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46d773-ed76-4a0b-b997-a3ac54c1221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM2 → overlay image → SigLIP CLS → (mask_token, mask) pair save\n",
    "\n",
    "import os, cv2, torch, numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "from transformers import SiglipVisionModel, SiglipImageProcessor\n",
    "\n",
    "root_ds   = Path(\"/home/s2behappy4/data/gyuhyeong/dataset/MMAD/MVTec-AD/hazelnut\")\n",
    "out_root  = Path(\"/home/s2behappy4/data/gyuhyeong/code/siglip_token/hazelnut\")\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "alpha     = 0.5\n",
    "save_overlay = False\n",
    "defects   = [\"crack\", \"cut\", \"hole\", \"print\"]\n",
    "\n",
    "sam_model = build_sam2(\n",
    "    \"configs/sam2.1/sam2.1_hiera_l.yaml\",\n",
    "    \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    ").to(device)\n",
    "\n",
    "sam_gen = SAM2AutomaticMaskGenerator(\n",
    "    model=sam_model,\n",
    "    points_per_side=64, pred_iou_thresh=0.6, stability_score_thresh=0.5,\n",
    "    mask_threshold=0.5, box_nms_thresh=0.7, crop_n_layers=2,\n",
    "    crop_overlap_ratio=0.2, crop_n_points_downscale_factor=1,\n",
    "    min_mask_region_area=300, output_mode=\"binary_mask\",\n",
    "    multimask_output=True, use_m2m=False\n",
    ")\n",
    "\n",
    "proc = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "vis  = SiglipVisionModel.from_pretrained(\"google/siglip-so400m-patch14-384\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1f5fa-c2de-4d19-b1eb-da2740ad20a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for defect in defects:\n",
    "    img_paths = sorted((root_ds / \"test\" / defect).glob(\"*.png\"))\n",
    "    out_dir   = out_root / defect / \"01\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if save_overlay:\n",
    "        ov_dir = Path(\"/home/s2behappy4/data/gyuhyeong/code/overlays/hazelnut\") / defect\n",
    "        ov_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total = len(img_paths)\n",
    "    for idx, img_path in enumerate(img_paths, 1):\n",
    "        print(f\"{defect:<5s} {idx:3d}/{total}  {img_path.name} 처리 중...\")\n",
    "\n",
    "        rgb = np.array(Image.open(img_path).convert(\"RGB\"))         \n",
    "        masks = sam_gen.generate(rgb)\n",
    "        if not masks:\n",
    "            print(\"실패\")\n",
    "            continue\n",
    "\n",
    "        pairs = []\n",
    "        for m_idx, ann in enumerate(masks):\n",
    "            seg = ann[\"segmentation\"].astype(bool)\n",
    "            overlay = rgb.copy().astype(np.float32)\n",
    "            red = np.array([255, 0, 0], dtype=np.float32)\n",
    "            overlay[seg] = (1 - alpha) * overlay[seg] + alpha * red\n",
    "            overlay = overlay.astype(np.uint8)\n",
    "            overlay_pil = Image.fromarray(overlay)\n",
    "\n",
    "            if save_overlay:\n",
    "                ov_name = f\"{img_path.stem}_m{m_idx:02d}_overlay.png\"\n",
    "                overlay_pil.save(ov_dir / ov_name)\n",
    "\n",
    "            batch = proc(images=overlay_pil, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                cls = vis(**batch).pooler_output.squeeze(0).cpu()\n",
    "\n",
    "            pairs.append({\"mask_token\": cls, \"mask\": torch.from_numpy(seg)})\n",
    "\n",
    "        torch.save(pairs, out_dir / f\"{img_path.stem}_pairs.pt\")\n",
    "        print(f\"   ↳ mask {len(pairs):2d}개 → {img_path.stem}_pairs.pt 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2e0fd-15a1-4f8a-ac6f-17ed9a99cff2",
   "metadata": {},
   "source": [
    "# # Good Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fdbfb-4b5e-4400-bec5-a96512cec5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "from transformers import SiglipVisionModel, SiglipImageProcessor\n",
    "\n",
    "root_ds   = Path(\"/home/s2behappy4/data/gyuhyeong/dataset/MMAD/MVTec-AD/hazelnut\")\n",
    "out_root  = Path(\"/home/s2behappy4/data/gyuhyeong/code/siglip_token/hazelnut/good/01\")\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "device    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "alpha     = 0.5\n",
    "\n",
    "sam_model = build_sam2(\n",
    "    \"configs/sam2.1/sam2.1_hiera_l.yaml\",\n",
    "    \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    ").to(device)\n",
    "\n",
    "sam_gen = SAM2AutomaticMaskGenerator(\n",
    "    model=sam_model,\n",
    "    points_per_side=64, pred_iou_thresh=0.6, stability_score_thresh=0.5,\n",
    "    mask_threshold=0.5, box_nms_thresh=0.7, crop_n_layers=2,\n",
    "    crop_overlap_ratio=0.2, crop_n_points_downscale_factor=1,\n",
    "    min_mask_region_area=300, output_mode=\"binary_mask\",\n",
    "    multimask_output=True, use_m2m=False\n",
    ")\n",
    "\n",
    "proc = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "vis  = SiglipVisionModel.from_pretrained(\"google/siglip-so400m-patch14-384\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca858cfc-1b9d-4ea7-9ef4-43a832198428",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dir = root_ds / \"test\" / \"good\"\n",
    "img_paths = sorted(good_dir.glob(\"*.png\"))\n",
    "print(f\"정상 이미지 {len(img_paths)}장 처리 시작 ...\")\n",
    "\n",
    "for idx, img_path in enumerate(img_paths, 1):\n",
    "    print(f\"good {idx:3d}/{len(img_paths)}  {img_path.name} 처리 중...\")\n",
    "\n",
    "    rgb = np.array(Image.open(img_path).convert(\"RGB\"))  \n",
    "    masks = sam_gen.generate(rgb)\n",
    "    if not masks:\n",
    "        print(\"실패\")\n",
    "        continue\n",
    "\n",
    "    pairs = []\n",
    "    for m_idx, ann in enumerate(masks):\n",
    "        seg = ann[\"segmentation\"].astype(bool)\n",
    "        overlay = rgb.copy().astype(np.float32)\n",
    "        red = np.array([255, 0, 0], dtype=np.float32)\n",
    "        overlay[seg] = (1 - alpha) * overlay[seg] + alpha * red\n",
    "        overlay = overlay.astype(np.uint8)\n",
    "        overlay_pil = Image.fromarray(overlay)\n",
    "\n",
    "        batch = proc(images=overlay_pil, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            cls = vis(**batch).pooler_output.squeeze(0).cpu() \n",
    "\n",
    "        pairs.append({\"mask_token\": cls, \"mask\": torch.from_numpy(seg)})\n",
    "\n",
    "    torch.save(pairs, out_root / f\"{img_path.stem}_pairs.pt\")\n",
    "    print(f\"   ↳ mask {len(pairs):2d}개 → {img_path.stem}_pairs.pt 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
